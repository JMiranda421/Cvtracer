{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In order to track and process the trial, you will now need to run another set script. This will involve the following steps:\n",
    "1. Import the relevant libraries from `cv-tracer`\n",
    "2. Re-open the appropriate `Trial` object.\n",
    "3. Enter parameters relevant to this run, and initialize a `CVTracer` object using those parameters. \n",
    "4. Run the tracking loop.\n",
    "5. Process the tracks to convert pixels to centimeters and calculate kinematic quantities like velocity, turning velocity, and acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the script\n",
    "\n",
    "First we can import the relevant libraries from `numpy` and `cv-tracer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import sys\n",
    "cvhome=\"/home/apatch/Code/cv-tracer\"\n",
    "sys.path.insert(0, cvhome)\n",
    "import argparse\n",
    "from TrAQ.Trial import Trial\n",
    "from TrAQ.CVTracer import CVTracer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will re-open the trial we initialized in the last step and input appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    parser = argparse.ArgumentParser(description=\"OpenCV2 Fish Tracker\")\n",
    "    parser.add_argument(\"raw_video\", type=str, help=\"path to raw video\")\n",
    "    parser.add_argument(\"-ts\",\"--t_start\", type=float, help=\"start time, in seconds\", default=0)\n",
    "    parser.add_argument(\"-te\",\"--t_end\", type=float, help=\"end time, in seconds\", default=-1)\n",
    "    parser.add_argument(\"-npix\",\"--n_pixel_blur\", type=float, help=\"square-root of n-pixels for threshold blurring\", default=3)\n",
    "    parser.add_argument(\"-bs\",\"--block_size\", type=int, help=\"contour block size\",default=15)\n",
    "    parser.add_argument(\"-th\",\"--thresh_offset\", type=int, help=\"threshold offset for contour-finding\",default=13) \n",
    "    parser.add_argument(\"-amin\",\"--min_area\", type=int, help=\"minimum area for threhold detection\", default=20)\n",
    "    parser.add_argument(\"-amax\",\"--max_area\", type=int, help=\"maximum area for threhold detection\", default=500)\n",
    "    parser.add_argument(\"-lt\",\"--trail_length\", type=int, help=\"length of trail\", default=3)\n",
    "    parser.add_argument(\"-on\",\"--online_viewer\", help=\"view tracking results in real time\", action='store_true') \n",
    "    parser.add_argument(\"-vs\",\"--view_scale\", help=\"to scale size of window for online viewing\", default=1.) \n",
    "    parser.add_argument(\"-RGB\", help=\"generate movie in RGB (default is greyscale)\", action='store_true')\n",
    "    parser.add_argument(\"-GPU\", help=\"use UMat file handling for OpenCV Transparent API\", action='store_true') \n",
    "    #parser.add_argument(\"-AMM\", help=\"switch to mean adaptive method instead of Gaussian\", action='store_true') \n",
    "    #parser.add_argument(\"-TI\",\"--thresh_invert\", help=\"Invert adaptive threshold\", action='store_true') \n",
    "    return parser.parse_args()\n",
    "\n",
    "args = arg_parse()\n",
    "# open up the Trial\n",
    "trial       = Trial(args.raw_video)\n",
    "# initialize a CVTracer\n",
    "frame_start = int(args.t_start * trial.fps)\n",
    "frame_end   = int(args.t_end   * trial.fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the `CVTracer` object. This object will allow us to step through each frame in a `Trial` video by using functions written to operate on `CVTracer` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt         = CVTracer( trial, frame_start = frame_start, frame_end = frame_end, \n",
    "                        n_pixel_blur = args.n_pixel_blur, block_size = args.block_size, \n",
    "                        threshold_offset = int(args.thresh_offset), \n",
    "                        min_area = args.min_area, max_area = args.max_area, \n",
    "                        len_trail = args.trail_length, \n",
    "                        RGB = args.RGB, online = args.online_viewer, view_scale = args.view_scale, \n",
    "                        GPU = args.GPU ) \n",
    "\n",
    "cvt.print_title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have initialized the `CVTracer` object `cvt`, we can run through each relevant frame and perform the necessary steps of tracking. \n",
    "\n",
    "For a deeper look at each of these functions, you can open up `cv-tracer/TrAQ/CVTracer.py`. Careful not to edit unless you know what you're doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt.set_frame(frame_start)\n",
    "for i_frame in range(cvt.frame_start, cvt.frame_end + 1):\n",
    "    # load next frame into video tracer\n",
    "    if cvt.get_frame():\n",
    "        # first remove all information from outside of tank\n",
    "        cvt.mask_tank()       \n",
    "        # detect contours in current frame\n",
    "        cvt.detect_contours()\n",
    "        # analyze contours for center and long axis\n",
    "        cvt.analyze_contours()\n",
    "        # estimate connection between last frame and current frame\n",
    "        cvt.connect_frames()\n",
    "        # store results from current frame in the trial object\n",
    "        cvt.update_trial()\n",
    "        # write frame with traces to new video file, show if online\n",
    "        cvt.draw()\n",
    "        # write frame to video output\n",
    "        cvt.write_frame()\n",
    "        # post frame to screen, may accept user input to break loop\n",
    "        if not cvt.post_frame(): break\n",
    "        # print time to terminal\n",
    "        cvt.print_current_frame()\n",
    "# put things away\n",
    "cvt.release()\n",
    "cvt.trial.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will convert the pixel data to centimeters and calculate some kinematic quantities. The `Trial` object knows how to convert to centimeters thanks to the radius we entered when we defined the `Tank`. It can convert frames to time using the frames per second we gave upon defining the `Trial`. With these two conversions, we are able to determine quantites like velocity and acceleration and director turning velocity and acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first convert pixels to centimeters\n",
    "trial.convert_pixels_to_cm()\n",
    "\n",
    "#trial.transform_lens()\n",
    "trial.calculate_kinematics()\n",
    "trial.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have completed this stage, you should check to see if the tracking worked. You can do this by opening `traced.mp4`, which should show the contours of the fish. There will still be errors, but the important thing is to ensure that the fish are tracked in most frames and in all portions of the arena. If not, please email apatch@fau.edu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
